{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression for predicting insurance claim cost\n",
    "\n",
    "In order to offer competitive prices and hence attract customers to an otherwise undifferentiated product, insurers need to be able to predict how much any given insurance claim will cost them.\n",
    "They also need to predict the cost of claims in advance so that they can offer their insurance packages without the risk of making a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd    # for reading csv data\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the dataset\n",
    "\n",
    "This dataset constists of \n",
    "\n",
    "rename the data file to `claim_costs.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.48831226e+01 6.61797085e+00 4.61127054e+01 ... 2.14450883e+00\n",
      "  1.37281866e+01 4.14286368e+03]\n",
      " [7.32642738e+01 9.97246823e+01 8.17092345e+01 ... 9.46795876e+01\n",
      "  3.45941237e+01 8.34806742e+03]\n",
      " [6.75670459e+00 6.29765119e+01 4.58118575e+01 ... 2.84445387e+01\n",
      "  2.94493562e+01 4.94751565e+03]\n",
      " ...\n",
      " [4.01979770e+01 2.92544351e+01 4.86059768e+01 ... 2.12417734e+01\n",
      "  6.19442140e+01 7.81426029e+03]\n",
      " [1.37961192e+01 7.34018502e+01 1.55845465e+01 ... 6.50788691e+00\n",
      "  8.85812544e+01 8.65646838e+03]\n",
      " [4.82804091e+01 4.76625752e+01 2.31347373e+01 ... 7.35452927e+01\n",
      "  8.18142284e+01 9.09558940e+03]]\n",
      "(321, 7)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('claim_costs.csv')\n",
    "#data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "data = np.array(data)     # convert to matrix\n",
    "\n",
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.56608880e-01 -4.02556176e-01 -2.51665483e-02 -3.14549158e-01\n",
      "  -5.06589672e-01 -3.62774752e-01]\n",
      " [ 2.38163618e-01  5.31369372e-01  3.34111445e-01  2.42400320e-01\n",
      "   4.23818305e-01 -1.53736255e-01]\n",
      " [-4.33854741e-01  1.62759476e-01 -2.82030256e-02 -1.28739846e-01\n",
      "  -2.42152035e-01 -2.05277408e-01]\n",
      " ...\n",
      " [-9.59511122e-02 -1.75496580e-01 -1.80488000e-06 -4.54418272e-03\n",
      "  -3.14573331e-01  1.20261590e-01]\n",
      " [-3.62725757e-01  2.67332924e-01 -3.33289165e-01  9.29868675e-02\n",
      "  -4.62717426e-01  3.87115979e-01]\n",
      " [-1.42830733e-02  9.14996194e-03 -2.57084617e-01  1.31357867e-01\n",
      "   2.11320333e-01  3.19322766e-01]]\n",
      "(321, 6)\n",
      "(321, 1)\n"
     ]
    }
   ],
   "source": [
    "# CENTER AROUND MEAN\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "features -= features.mean(axis=0)\n",
    "\n",
    "# DIVIDE BY RANGE\n",
    "max_features = features.max(axis=0)\n",
    "min_features = features.min(axis=0)\n",
    "\n",
    "ranges = max_features - min_features\n",
    "\n",
    "features /= ranges\n",
    "print(features)\n",
    "\n",
    "print(features.shape)\n",
    "labels = np.reshape(labels, (features.shape[0], 1))\n",
    "print(labels.shape)\n",
    "\n",
    "normalised_data = np.hstack((features, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slitting the dataset into test, train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "(256, 7)\n",
      "(48, 7)\n",
      "(17, 7)\n"
     ]
    }
   ],
   "source": [
    "n_data = normalised_data\n",
    "shuffle(n_data)\n",
    "\n",
    "print(len(n_data))\n",
    "\n",
    "train_size =int( 0.8 * len(n_data) )\n",
    "val_size = int( 0.15 * len(n_data) )\n",
    "test_size = len(n_data) - train_size - val_size\n",
    "\n",
    "train_data = n_data[ : train_size]\n",
    "val_data = n_data[train_size : train_size + val_size]\n",
    "test_data = n_data[train_size + val_size :]\n",
    "\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.batches = []\n",
    "        i = 0\n",
    "        while i + batch_size < len(dataset):\n",
    "            self.batches.append(dataset[i:i+batch_size])\n",
    "            i += batch_size\n",
    "        self.batches.append(dataset[i:])\n",
    "        #shuffle(self.batches)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            shuffle(self.batches)\n",
    "        #print(type(self.batches))\n",
    "        batch = self.batches[idx]\n",
    "        features = batch[:, :-1]\n",
    "        labels = batch[:, -1]\n",
    "        return features, labels\n",
    "\n",
    "train_loader = DataLoader(train_data, 5)\n",
    "val_loader = DataLoader(val_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "We are going to creat a function to map our inputs to our output (confidence of transaction being false)\n",
    "We will use a linear model as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "class LinearModel():\n",
    "    \n",
    "    def __init__(self, n_features=6, n_outputs=1):\n",
    "        self.weights = np.random.randn(n_outputs, n_features)    \n",
    "        self.biases = np.random.randn(n_outputs)\n",
    "        print(self.weights.shape)\n",
    "        print(self.biases.shape)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = np.matmul(self.weights, x.T)\n",
    "        x += self.biases\n",
    "        return x\n",
    "    \n",
    "linear_model = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the loss function\n",
    "\n",
    "The loss function is a measure of how badly the model is currently performing. \n",
    "\n",
    "# $L = \\frac{1}{2m} \\sum^m_{i=1} (\\hat{y} - y)^2 = \\frac{1}{2m} \\sum^m_{i=1} ( (WX + b) - y)^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_fn(model, batch, gradient=False):\n",
    "    x, y = batch\n",
    "    #print('y:', y)\n",
    "    #print(y.shape)\n",
    "    \n",
    "    y_hat = model(x)\n",
    "    \n",
    "    #print('y_hat:', y_hat)\n",
    "    #print(y_hat.shape)\n",
    "    \n",
    "    if gradient == False:\n",
    "        loss = 0.5 * np.mean( np.power( (y_hat - y), 2 ) )\n",
    "        print(loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    if gradient == True:\n",
    "        z = model(x)\n",
    "        print('z', (z-y).shape)\n",
    "        print(x.shape)\n",
    "        \n",
    "        w_grad = (np.matmul( ( z - y ), x ) )\n",
    "        print(w_grad)\n",
    "        \n",
    "        b_grad = np.mean( z - y )\n",
    "        return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the optimiser\n",
    "The optimiser will update the parameters (weights and biases) of the model in a direction that reduces the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SGDOptimiser():\n",
    "    \n",
    "    def __init__(self, model, lr=0.1):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self, grad):\n",
    "        w_grad, b_grad = grad\n",
    "        self.model.weights -= self.lr * w_grad        \n",
    "        self.model.biases -= self.lr * b_grad\n",
    "        \n",
    "optimiser = SGDOptimiser(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28149684.94726901\n",
      "Epoch: 0 \tBatch: 0 \tLoss: 28149684.94726901\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1445.61194158  4564.42391924  5322.40271516 13541.54504963\n",
      "   6148.38124833 -5758.43760274]]\n",
      "\tValidating\n",
      "22424496.771684255\n",
      "34094361.0428953\n",
      "34238587.65258358\n",
      "31870431.11101962\n",
      "29146039.990949027\n",
      "28644191.02171714\n",
      "15216676.341830527\n",
      "26969686.252141763\n",
      "18494609.90524366\n",
      "28083761.773887444\n",
      "\tAverage batch loss:\n",
      "18757443.5601004\n",
      "Epoch: 0 \tBatch: 1 \tLoss: 18757443.5601004\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3946.38481263 -2924.93221629 -4216.51894138  3369.93334903\n",
      "   6444.96752553  -607.70567022]]\n",
      "25315316.01840533\n",
      "Epoch: 0 \tBatch: 2 \tLoss: 25315316.01840533\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  -248.78146572  -7402.96928426   -930.46412301  -5540.43683718\n",
      "  -10444.85449654     15.41840932]]\n",
      "12637054.392438065\n",
      "Epoch: 0 \tBatch: 3 \tLoss: 12637054.392438065\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  492.27227666 -1012.43068873  1267.11666442   487.35203409\n",
      "   5264.03237343 -2917.96918921]]\n",
      "8179117.0005160915\n",
      "Epoch: 0 \tBatch: 4 \tLoss: 8179117.0005160915\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-4012.06188904  4227.25771447  -477.73971533  2088.02177479\n",
      "   3389.14039972  -763.83498663]]\n",
      "16256871.405427769\n",
      "Epoch: 0 \tBatch: 5 \tLoss: 16256871.405427769\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1251.83744503  1405.66719634 -1266.6144713  -3356.81536099\n",
      "   -668.23558273 -3568.19287899]]\n",
      "\tValidating\n",
      "7852285.933077228\n",
      "11254129.591340467\n",
      "14834779.397339353\n",
      "11716686.712165887\n",
      "13336655.387272757\n",
      "16337197.736565942\n",
      "5582169.038304645\n",
      "8111858.221287678\n",
      "16212039.486684965\n",
      "12444126.706930399\n",
      "\tAverage batch loss:\n",
      "3256604.9191806167\n",
      "Epoch: 0 \tBatch: 6 \tLoss: 3256604.9191806167\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -391.60788814 -1716.28638699  2000.14382424  1631.73449641\n",
      "  -1243.98890951  -938.49030913]]\n",
      "7621828.7263122145\n",
      "Epoch: 0 \tBatch: 7 \tLoss: 7621828.7263122145\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  889.92582487  4077.77608834  1269.24642592 -1083.86464985\n",
      "  -3886.11072265 -2051.85127443]]\n",
      "9151905.994595006\n",
      "Epoch: 0 \tBatch: 8 \tLoss: 9151905.994595006\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-5844.08931367 -4336.60053052  1240.48923176 -2274.32081801\n",
      "   -267.58054097 -2224.930008  ]]\n",
      "6572512.104148595\n",
      "Epoch: 0 \tBatch: 9 \tLoss: 6572512.104148595\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-2716.69071108 -3016.99243793 -4311.95856571 -3138.13357929\n",
      "   -662.82915597   575.72847308]]\n",
      "9327756.72049968\n",
      "Epoch: 0 \tBatch: 10 \tLoss: 9327756.72049968\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  651.25872102   389.07111796   731.66854676 -1865.82366981\n",
      "   2688.97852299 -6241.47181786]]\n",
      "\tValidating\n",
      "7851676.578812353\n",
      "2027919.9911898572\n",
      "7777122.158632531\n",
      "5342714.633642023\n",
      "5063616.381846226\n",
      "2739973.2660492817\n",
      "7443967.108512772\n",
      "4267573.7297549965\n",
      "3106806.824409727\n",
      "5647746.069729502\n",
      "\tAverage batch loss:\n",
      "2993327.888213427\n",
      "Epoch: 0 \tBatch: 11 \tLoss: 2993327.888213427\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1456.44901302 -4768.70345392 -3124.51002535 -2705.92746339\n",
      "  -3125.71538023    53.42168501]]\n",
      "7386133.016236201\n",
      "Epoch: 0 \tBatch: 12 \tLoss: 7386133.016236201\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1267.77331565   904.87269866 -5877.6133941   1788.74589411\n",
      "    751.64669015 -3483.16850532]]\n",
      "3160986.563925614\n",
      "Epoch: 0 \tBatch: 13 \tLoss: 3160986.563925614\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 1159.92688387 -2207.42583281 -1823.29952539    22.50565954\n",
      "   -458.29166037  -815.36910073]]\n",
      "1639917.2283364176\n",
      "Epoch: 0 \tBatch: 14 \tLoss: 1639917.2283364176\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[   -4.36050827   940.3075974  -1517.592328   -1505.22567761\n",
      "   1247.78060991 -1709.26839355]]\n",
      "1275299.4721877214\n",
      "Epoch: 0 \tBatch: 15 \tLoss: 1275299.4721877214\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -714.84529916 -1891.84186464   217.67636087   109.13363221\n",
      "   -220.74231807 -1539.39004795]]\n",
      "\tValidating\n",
      "3300114.4047549576\n",
      "2605435.0297901025\n",
      "3190789.8721362753\n",
      "3609513.9361694953\n",
      "1689509.7328655254\n",
      "3722770.8529443135\n",
      "1203751.9297542605\n",
      "1595303.6589556714\n",
      "1659461.0805761493\n",
      "4246861.624653997\n",
      "\tAverage batch loss:\n",
      "725889.343914679\n",
      "Epoch: 0 \tBatch: 16 \tLoss: 725889.343914679\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-527.44583654 -771.7761661  -568.26885306 -713.06665952 -874.73586696\n",
      "   804.95509448]]\n",
      "4799699.956296554\n",
      "Epoch: 0 \tBatch: 17 \tLoss: 4799699.956296554\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-4993.2995764   3105.17561591 -1708.25373584 -3973.90097545\n",
      "  -1906.36370241 -4072.22995504]]\n",
      "990336.7363843605\n",
      "Epoch: 0 \tBatch: 18 \tLoss: 990336.7363843605\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  800.3210112  -1393.23977278  -415.73947259 -1639.31969236\n",
      "    246.46070733 -1177.75835343]]\n",
      "769200.9772102878\n",
      "Epoch: 0 \tBatch: 19 \tLoss: 769200.9772102878\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-602.39005007  474.10833458 -476.40012783  -55.71329913 -322.17721813\n",
      "  -814.83616702]]\n",
      "1553168.955993511\n",
      "Epoch: 0 \tBatch: 20 \tLoss: 1553168.955993511\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1416.18381451   296.51440118  -773.43748856 -1443.56384495\n",
      "    190.23263152 -2464.37332092]]\n",
      "\tValidating\n",
      "2576831.0110512716\n",
      "984632.9653330207\n",
      "595603.1650818337\n",
      "1697272.0798071332\n",
      "626735.1955377089\n",
      "616106.5778515999\n",
      "2303690.487008917\n",
      "1571306.9352274137\n",
      "2027374.217458196\n",
      "1685189.0761065758\n",
      "\tAverage batch loss:\n",
      "907733.400109844\n",
      "Epoch: 0 \tBatch: 21 \tLoss: 907733.400109844\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1741.22541663  -681.63664896   769.48900222  -442.07143587\n",
      "   1021.00267026 -1654.8792313 ]]\n",
      "958102.9976049907\n",
      "Epoch: 0 \tBatch: 22 \tLoss: 958102.9976049907\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -560.57902102 -1527.95847022   -71.31382091  -389.71251905\n",
      "  -1114.30791277 -1122.09275929]]\n",
      "234659.82566343714\n",
      "Epoch: 0 \tBatch: 23 \tLoss: 234659.82566343714\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 422.91476728  148.90958347 -697.81106201  -33.43337225  664.59414542\n",
      "    -4.95661465]]\n",
      "260146.646978606\n",
      "Epoch: 0 \tBatch: 24 \tLoss: 260146.646978606\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  362.26411256 -1152.12580415  -397.17569295  -821.32436214\n",
      "   -425.48502152  -653.85642667]]\n",
      "782569.0083605307\n",
      "Epoch: 0 \tBatch: 25 \tLoss: 782569.0083605307\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -134.42778049    61.50501641   252.3209048   -712.20850396\n",
      "    104.46637069 -1477.48555123]]\n",
      "\tValidating\n",
      "589058.0346595931\n",
      "1385026.4742511122\n",
      "1081423.1286306512\n",
      "318304.31168895273\n",
      "418725.5038180897\n",
      "1713942.4413871646\n",
      "1042293.1761142418\n",
      "994636.711343473\n",
      "416762.5233863882\n",
      "1791705.639454168\n",
      "\tAverage batch loss:\n",
      "1928541.1928159166\n",
      "Epoch: 0 \tBatch: 26 \tLoss: 1928541.1928159166\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -309.95722241   834.1132028  -1791.54572462   291.14537715\n",
      "  -2009.61223658 -3228.55846792]]\n",
      "861968.3320628416\n",
      "Epoch: 0 \tBatch: 27 \tLoss: 861968.3320628416\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 4.09815549e+01  1.04150248e+00 -1.74897598e+03  3.55958104e+01\n",
      "   4.04190901e+02 -1.10310681e+03]]\n",
      "422902.3690615509\n",
      "Epoch: 0 \tBatch: 28 \tLoss: 422902.3690615509\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 891.34225791  600.43125039 -322.59736949 -859.06325447 -894.89644636\n",
      "   -62.67703802]]\n",
      "68664.70201096182\n",
      "Epoch: 0 \tBatch: 29 \tLoss: 68664.70201096182\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -84.05769591  184.34617663 -248.44155728 -448.49179947  299.25597413\n",
      "  -291.55474803]]\n",
      "484889.6202292891\n",
      "Epoch: 0 \tBatch: 30 \tLoss: 484889.6202292891\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  -25.76072053  -192.96787141  -581.99744776   323.81909309\n",
      "   1215.05656772 -1220.02661053]]\n",
      "\tValidating\n",
      "830365.7874713894\n",
      "671583.718778922\n",
      "1037659.3869050512\n",
      "990243.003919395\n",
      "573076.2765039967\n",
      "537903.4726966479\n",
      "336940.2718838832\n",
      "203932.07955744298\n",
      "281968.72878816957\n",
      "247009.6095710094\n",
      "\tAverage batch loss:\n",
      "691460.7000463986\n",
      "Epoch: 0 \tBatch: 31 \tLoss: 691460.7000463986\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  535.7256508    913.90993927 -1223.31321717   129.38118121\n",
      "    399.38303696 -1026.85926919]]\n",
      "426174.70907478395\n",
      "Epoch: 0 \tBatch: 32 \tLoss: 426174.70907478395\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1338.02788255   271.42132562  -520.12200844  -476.9599701\n",
      "   -812.79366735 -1085.17549935]]\n",
      "298343.5963840814\n",
      "Epoch: 0 \tBatch: 33 \tLoss: 298343.5963840814\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[   33.3816617     40.74128019   139.27721111    29.66671316\n",
      "     81.39864671 -1259.21445492]]\n",
      "391970.57941048907\n",
      "Epoch: 0 \tBatch: 34 \tLoss: 391970.57941048907\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  321.48445347   839.02368187  -516.80090549   321.3843951\n",
      "  -1078.72570125  -673.55646525]]\n",
      "162847.8205218259\n",
      "Epoch: 0 \tBatch: 35 \tLoss: 162847.8205218259\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 472.88291591  515.63865763 -278.12278757   75.19584464 -429.95171562\n",
      "  -133.72491257]]\n",
      "\tValidating\n",
      "208630.76039248955\n",
      "167580.9612972062\n",
      "229672.87815235928\n",
      "308648.1154919449\n",
      "518232.61993872\n",
      "632265.481710178\n",
      "159009.42823205492\n",
      "655944.8381041726\n",
      "446159.1392619802\n",
      "384944.1147374103\n",
      "\tAverage batch loss:\n",
      "280016.4538998101\n",
      "Epoch: 0 \tBatch: 36 \tLoss: 280016.4538998101\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  306.97917367   -90.32093985 -1124.3582414   -478.27763765\n",
      "   -585.19136686   -26.32534477]]\n",
      "28296.355403125966\n",
      "Epoch: 0 \tBatch: 37 \tLoss: 28296.355403125966\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -61.82344985  -82.00155862  127.72856926   96.32987223  128.9668766\n",
      "  -156.32062629]]\n",
      "391844.6890921383\n",
      "Epoch: 0 \tBatch: 38 \tLoss: 391844.6890921383\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -230.11794473   171.36455347  -787.53326033   756.22494058\n",
      "     82.94274742 -1144.53388957]]\n",
      "405112.57677503827\n",
      "Epoch: 0 \tBatch: 39 \tLoss: 405112.57677503827\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  687.34224961 -1018.69840302  -455.89694371   121.83926072\n",
      "    139.29698274 -1526.18938089]]\n",
      "49952.43435599057\n",
      "Epoch: 0 \tBatch: 40 \tLoss: 49952.43435599057\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-312.65377527  295.26153207  178.63907209 -323.67000879 -248.77831161\n",
      "  -187.46245696]]\n",
      "\tValidating\n",
      "321306.04142845905\n",
      "407691.8591493185\n",
      "154748.6280580535\n",
      "135556.61742164285\n",
      "171104.28244876635\n",
      "149446.6729711299\n",
      "164994.99505033696\n",
      "369961.12602329766\n",
      "324394.68602129584\n",
      "228889.20652095723\n",
      "\tAverage batch loss:\n",
      "553919.8750395277\n",
      "Epoch: 0 \tBatch: 41 \tLoss: 553919.8750395277\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  -86.09888881   242.0013121  -1822.25611544  1092.44770606\n",
      "    161.90972907 -1776.96123994]]\n",
      "322155.2209777787\n",
      "Epoch: 0 \tBatch: 42 \tLoss: 322155.2209777787\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  664.3330881  -1323.14460762  -837.24495604 -1342.90909061\n",
      "   -342.85511265 -1565.33347814]]\n",
      "130739.5770265443\n",
      "Epoch: 0 \tBatch: 43 \tLoss: 130739.5770265443\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 340.6432832   438.05352381 -334.98852195  402.86573003  513.76322094\n",
      "  -347.02701131]]\n",
      "65394.82654017342\n",
      "Epoch: 0 \tBatch: 44 \tLoss: 65394.82654017342\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -57.26343165 -181.50424311  -77.55286701  551.17653285  -97.37033388\n",
      "  -355.90761347]]\n",
      "70828.41331009092\n",
      "Epoch: 0 \tBatch: 45 \tLoss: 70828.41331009092\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  43.33373825  460.24086167  119.78962273  146.13657071  345.18514561\n",
      "  -444.03652324]]\n",
      "\tValidating\n",
      "104272.02770884127\n",
      "156653.01922756093\n",
      "243256.85212563965\n",
      "283845.0724689901\n",
      "88780.04267477406\n",
      "115045.53181783626\n",
      "101120.06295162051\n",
      "226565.0616095302\n",
      "250988.8072259781\n",
      "121626.92474310631\n",
      "\tAverage batch loss:\n",
      "229099.49946388052\n",
      "Epoch: 0 \tBatch: 46 \tLoss: 229099.49946388052\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 650.99892337  386.44192812 -906.46021256 -334.16503363  -96.2006743\n",
      "  -669.52987494]]\n",
      "5141.596249712772\n",
      "Epoch: 0 \tBatch: 47 \tLoss: 5141.596249712772\n",
      "z (1, 1)\n",
      "(1, 6)\n",
      "[[  8.57560918  36.46478909  49.96318626 -36.02139251  -1.30209267\n",
      "  -20.75593825]]\n",
      "56501.97239177416\n",
      "Epoch: 0 \tBatch: 48 \tLoss: 56501.97239177416\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[  64.50471076  292.7811072    -0.52395286 -176.42430694  446.59142723\n",
      "  -421.03915405]]\n",
      "149346.04810052383\n",
      "Epoch: 0 \tBatch: 49 \tLoss: 149346.04810052383\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ -31.95676741  308.46530167 -441.73719178  631.19326145 -148.62934859\n",
      "  -716.39955329]]\n",
      "133377.59674012117\n",
      "Epoch: 0 \tBatch: 50 \tLoss: 133377.59674012117\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 168.85587267  288.30619162  230.59031273 -156.20099748 -471.24773628\n",
      "  -604.68622287]]\n",
      "\tValidating\n",
      "66366.18265456606\n",
      "89550.69564350908\n",
      "181977.65429998483\n",
      "60895.613565705135\n",
      "216231.2753352827\n",
      "115599.8198994428\n",
      "83765.18283831209\n",
      "95125.5919363483\n",
      "181876.68340253015\n",
      "159771.960456803\n",
      "\tAverage batch loss:\n",
      "177783.6908083139\n",
      "Epoch: 0 \tBatch: 51 \tLoss: 177783.6908083139\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 411.82078801  575.00408914 -207.20796899 -319.00338857 -489.97781483\n",
      "  -643.90392853]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyP4T6lyCv+oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(model, epochs=1):\n",
    "    \n",
    "    e = []        # just a list to store the batch_idxs where validation happened\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            loss = loss_fn(model, batch)\n",
    "            print('Epoch:', epoch, '\\tBatch:', batch_idx, '\\tLoss:', loss)\n",
    "            w_grad, b_grad = loss_fn(model, batch, gradient=True)\n",
    "            optimiser.step((w_grad, b_grad))\n",
    "            \n",
    "            train_losses.append(loss)\n",
    "\n",
    "            if batch_idx % 5 == 0:\n",
    "                print('\\tValidating')\n",
    "                batch_losses = []\n",
    "                for val_batch in val_loader:\n",
    "                    l = loss_fn(model, val_batch)\n",
    "                    batch_losses.append(l)\n",
    "                avg_batch_loss = np.mean(batch_losses)\n",
    "                print('\\tAverage batch loss:', )\n",
    "                e.append(batch_idx)\n",
    "                val_losses.append(avg_batch_loss)\n",
    "                \n",
    "    plt.plot(train_losses, 'b')\n",
    "    plt.plot(e, val_losses)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "            \n",
    "train(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing the model\n",
    "\n",
    "Now we have seen our model learn from the training set, validated this with the validation set and used that to adjust the hyperparameters. Now we need to test the model by seeing how it performs on unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
